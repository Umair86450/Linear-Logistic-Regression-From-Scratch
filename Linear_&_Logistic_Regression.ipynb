{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Complete Linear Regression Notes\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 1. What is Linear Regression?\n",
        "\n",
        "Linear Regression is a supervised learning algorithm used to predict a continuous output based on one or more input variables.  \n",
        "It finds the best-fitting straight line (linear relationship) that minimizes the difference between actual and predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 2. Linear Regression Equation:\n",
        "\n",
        "For Simple Linear Regression:\n",
        "\n",
        "\\[\n",
        "y = a + bX + \\varepsilon\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- **y** is the dependent variable (output),  \n",
        "- **a** is the intercept,  \n",
        "- **b** is the slope coefficient (weight),  \n",
        "- **X** is the independent variable (input),  \n",
        "- **ε (epsilon)** is the error term (noise).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 3. Outcomes of Linear Regression:\n",
        "\n",
        "- ✅ **Predictions (ŷ):**  \n",
        "  The model gives predicted values based on the input features.\n",
        "\n",
        "- ✅ **Coefficients (b):**  \n",
        "  Represent how much the output changes per unit change in input.\n",
        "\n",
        "- ✅ **Intercept (a):**  \n",
        "  The value of output when all inputs are 0.\n",
        "\n",
        "- ✅ **Residuals:**  \n",
        "  The difference between actual and predicted values:  \n",
        "  \\[\n",
        "  \\text{Residual} = y - \\hat{y}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 4. Evaluation Metrics:\n",
        "\n",
        "These metrics tell you how good your model is.\n",
        "\n",
        "- ✅ **(a) MAE – Mean Absolute Error**  \n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "  Measures the average magnitude of errors.  \n",
        "  Less sensitive to outliers.  \n",
        "  Units are same as the target variable.\n",
        "\n",
        "- ✅ **(b) MSE – Mean Squared Error**  \n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "  Penalizes larger errors more (because of squaring).  \n",
        "  Not interpretable in real units (units are squared).\n",
        "\n",
        "- ✅ **(c) RMSE – Root Mean Squared Error**  \n",
        "$$\n",
        "RMSE = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "  Same units as the target variable.  \n",
        "  Useful when large errors need to be penalized.\n",
        "\n",
        "- ✅ **(d) R² – Coefficient of Determination**  \n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "  Where:  \n",
        "  - Numerator = Sum of Squared Errors (SSE)  \n",
        "  - Denominator = Total Sum of Squares (SST)\n",
        "\n",
        "📌 **Interpretation:**  \n",
        "- \\(R^2 = 1\\) → Perfect model  \n",
        "- \\(R^2 = 0\\) → Model explains nothing  \n",
        "- \\(R^2 < 0\\) → Worse than predicting the mean\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 5. MAE vs MSE vs RMSE – When to Use What?\n",
        "\n",
        "| Metric | Use When | Sensitive to Outliers? | Units |\n",
        "|--------|-----------|-----------------------|-------|\n",
        "| MAE    | Simpler interpretation needed | ❌ No | Same as target |\n",
        "| MSE    | You want to penalize large errors | ✅ Yes | Squared units |\n",
        "| RMSE   | Want to penalize big errors but keep original units | ✅ Yes | Same as target |\n",
        "| R²     | Want to know % variance explained | ❌ No | No units (ratio) |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ucfZpdqVhycX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **🧮 Linear Regression in Pure Python (No Libraries)**\n",
        "\n",
        "This script implements Linear Regression using Gradient Descent, built using only core Python. It also calculates important metrics like MSE, MAE, and R² Score to evaluate the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "qzCAEPyfZgk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊 Data"
      ],
      "metadata": {
        "id": "fJOT_2t3Z8Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1, 2, 3, 4, 5]\n",
        "y = [3, 5, 7, 9, 11]\n"
      ],
      "metadata": {
        "id": "mNowzIOFZgPu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**x**: Input features (e.g., hours studied)  \n",
        "**y**: Target labels (e.g., scores)\n",
        "\n",
        "The data follows a linear trend:\n",
        "\n",
        "**y = 2x + 1**\n"
      ],
      "metadata": {
        "id": "mh5tugUuaO_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚙️ Initialization"
      ],
      "metadata": {
        "id": "VRSLhEwsaR2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 0.0  # weight (slope)\n",
        "b = 0.0  # bias (intercept)\n"
      ],
      "metadata": {
        "id": "e6c8Dz0iZgNd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with 0 values for both parameters  \n",
        "- These will be learned through training\n"
      ],
      "metadata": {
        "id": "v2HC4Z8iabj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "max_iterations = 10000\n",
        "tolerance = 1e-6\n",
        "n = len(x)\n"
      ],
      "metadata": {
        "id": "JeZ4AgT9ZgLC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **learning_rate**: How big each parameter update step is  \n",
        "- **max_iterations**: Maximum number of training loops  \n",
        "- **tolerance**: Training stops when improvement is too small  \n",
        "- **n**: Number of training samples\n"
      ],
      "metadata": {
        "id": "6tRfX5JTak6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧮 Error Metrics\n",
        "## 📌 MSE (Mean Squared Error)"
      ],
      "metadata": {
        "id": "WufMLoiRanq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return sum((y_true[i] - y_pred[i])**2 for i in range(n)) / n\n"
      ],
      "metadata": {
        "id": "FwNyCSJwZgIG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures average squared difference between actual and predicted values  \n",
        "- Penalizes large errors more\n"
      ],
      "metadata": {
        "id": "rPHWtG0RawBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 MAE (Mean Absolute Error)"
      ],
      "metadata": {
        "id": "5aB3xPzHazCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(y_true, y_pred):\n",
        "    return sum(abs(y_true[i] - y_pred[i]) for i in range(n)) / n\n"
      ],
      "metadata": {
        "id": "ijqYExAPZgEm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures average absolute difference\n",
        "\n",
        "- Easier to interpret, less sensitive to outliers than MSE\n",
        "\n"
      ],
      "metadata": {
        "id": "TP2-qm2Qa2_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 R² Score (Coefficient of Determination)"
      ],
      "metadata": {
        "id": "wkmEy1Oza9Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r2_score(y_true, y_pred):\n",
        "    y_mean = sum(y_true) / n\n",
        "    ss_tot = sum((y_true[i] - y_mean)**2 for i in range(n))\n",
        "    ss_res = sum((y_true[i] - y_pred[i])**2 for i in range(n))\n",
        "    return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n"
      ],
      "metadata": {
        "id": "A7f080QlZf-k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Shows how much variation in **y** is explained by the model  \n",
        "- Closer to **1** means better prediction\n"
      ],
      "metadata": {
        "id": "wUxItKTPbGMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔁 Gradient Descent"
      ],
      "metadata": {
        "id": "ojb181iObIwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev_mse = float('inf')\n",
        "iteration = 0\n"
      ],
      "metadata": {
        "id": "RNNSnUPpbGAB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We track `prev_mse` to check for convergence  \n",
        "- Begin training with `iteration = 0`\n"
      ],
      "metadata": {
        "id": "Wj4ChgAhbP-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 Training Loop"
      ],
      "metadata": {
        "id": "K7pEHb-EeEH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while iteration < max_iterations:\n",
        "    # Step 1: Predict using current w and b\n",
        "    y_pred = [w * x[i] + b for i in range(n)]\n",
        "\n",
        "    # Step 2: Calculate Gradients (dw and db)\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    for i in range(n):\n",
        "        error = y[i] - y_pred[i]          # Difference between actual and predicted\n",
        "        dw += -2 * x[i] * error           # Gradient of MSE w.r.t. w\n",
        "        db += -2 * error                  # Gradient of MSE w.r.t. b\n",
        "    dw /= n                               # Average gradient for weight\n",
        "    db /= n                               # Average gradient for bias\n",
        "\n",
        "    # Step 3: Update Parameters\n",
        "    w = w - learning_rate * dw            # Move weight against the gradient\n",
        "    b = b - learning_rate * db            # Move bias against the gradient\n",
        "\n",
        "    # Step 4: Calculate Performance Metrics\n",
        "    current_mse = mse(y, y_pred)          # Mean Squared Error\n",
        "    current_mae = mae(y, y_pred)          # Mean Absolute Error\n",
        "    current_r2 = r2_score(y, y_pred)      # R² Score\n",
        "\n",
        "    # Step 5: Log progress every 500 iterations\n",
        "    if iteration % 500 == 0:\n",
        "        print(f\"Iter {iteration}: w={w:.4f}, b={b:.4f}, MSE={current_mse:.6f}, MAE={current_mae:.6f}, R2={current_r2:.6f}\")\n",
        "\n",
        "    # Step 6: Check for Convergence\n",
        "    if abs(prev_mse - current_mse) < tolerance:\n",
        "        print(f\"Converged at iteration {iteration}\")\n",
        "        break\n",
        "\n",
        "    prev_mse = current_mse                # Store current MSE to compare in next loop\n",
        "    iteration += 1                        # Move to next iteration\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRPyOPxaeD5k",
        "outputId": "1471bb6d-d660-4493-b43a-779924f51ce6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0: w=0.5000, b=0.1400, MSE=57.000000, MAE=7.000000, R2=-6.125000\n",
            "Iter 500: w=2.0210, b=0.9241, MSE=0.001056, MAE=0.027897, R2=0.999868\n",
            "Converged at iteration 792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Explanation (Line by Line)\n",
        "\n",
        "| Section                                      | What It Does                                                                 |\n",
        "|---------------------------------------------|------------------------------------------------------------------------------|\n",
        "| `while iteration < max_iterations:`         | Loop runs until either max iterations reached or early stopping is triggered. |\n",
        "| `y_pred = [w * x[i] + b for i in range(n)]`  | Predicts output using current `w` and `b`.                                  |\n",
        "| `dw, db`                                     | Initialize gradients to 0 before accumulation.                              |\n",
        "| `error = y[i] - y_pred[i]`                   | Measures how far prediction is from actual value.                           |\n",
        "| `dw += -2 * x[i] * error`                    | Gradient of MSE with respect to weight `w`.                                 |\n",
        "| `db += -2 * error`                           | Gradient of MSE with respect to bias `b`.                                   |\n",
        "| `dw /= n, db /= n`                           | Average gradient across all samples.                                        |\n",
        "| `w -= learning_rate * dw`                    | Update the weight by moving opposite the gradient.                          |\n",
        "| `b -= learning_rate * db`                    | Update the bias similarly.                                                  |\n",
        "| `current_mse = mse(...)`                     | Calculate Mean Squared Error for tracking.                                  |\n",
        "| `current_mae = mae(...)`                     | Calculate Mean Absolute Error.                                              |\n",
        "| `current_r2 = r2_score(...)`                 | R² measures how well the model explains the variance.                       |\n",
        "| `if iteration % 500 == 0:`                   | Print progress every 500 iterations.                                        |\n",
        "| `if abs(prev_mse - current_mse) < tolerance:`| Stop if error improvement is too small.                                     |\n",
        "| `prev_mse = current_mse`                     | Prepare for next loop comparison.                                           |\n",
        "| `iteration += 1`                             | Count the iteration.                                                        |\n",
        "\n",
        "---\n",
        "\n",
        "## 🏁 Summary\n",
        "\n",
        "**Goal:** Minimize MSE using Gradient Descent\n",
        "\n",
        "**Update rule:**\n",
        "\n",
        "w = w − α ⋅ ∂Loss/∂w␣␣  \n",
        "b = b − α ⋅ ∂Loss/∂b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Early Stopping** ensures we don’t keep training when improvement is negligible.\n",
        "- **Metrics** (`MSE`, `MAE`, `R²`) help monitor the learning progress.\n"
      ],
      "metadata": {
        "id": "kwBkGojFejFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Final Model Output (Markdown Format)"
      ],
      "metadata": {
        "id": "ylDR2sT0hDlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinal Model:\")\n",
        "print(f\"y = {w:.4f} * x + {b:.4f}\")\n",
        "print(f\"Final MSE: {current_mse:.6f}\")\n",
        "print(f\"Final MAE: {current_mae:.6f}\")\n",
        "print(f\"Final R2 Score: {current_r2:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvj-mCB3bF8p",
        "outputId": "335145f3-45d2-408c-b156-bab422425642"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model:\n",
            "y = 2.0078 * x + 0.9718\n",
            "Final MSE: 0.000146\n",
            "Final MAE: 0.010377\n",
            "Final R2 Score: 0.999982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📄 Explanation:\n",
        "\n",
        "| Line of Code                                    | Description                                                                                               |\n",
        "|------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| `print(\"\\nFinal Model:\")`                        | Adds a line break and labels the final output section.                                                    |\n",
        "| `print(f\"y = {w:.4f} * x + {b:.4f}\")`           | Prints the final learned linear regression equation with weight (`w`) and bias (`b`) rounded to 4 decimals. |\n",
        "| `print(f\"Final MSE: {current_mse:.6f}\")`         | Displays the final Mean Squared Error — a measure of how far predictions are from actual values (lower is better). |\n",
        "| `print(f\"Final MAE: {current_mae:.6f}\")`         | Displays the final Mean Absolute Error — shows the average magnitude of errors (no direction).             |\n",
        "| `print(f\"Final R2 Score: {current_r2:.6f}\")`     | Shows the final R² Score — closer to 1 means a better fit.                                                |\n"
      ],
      "metadata": {
        "id": "wk-hWzOshQBy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZ9JVOX7bF52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyu5RuQzbF3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXSBoY73bF0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3tV_uVemlYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEy1D0fYmlU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Md6bBK25mlSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuMp6yinmlO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9_6WXBrmlL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Complete Logistic Regression  Notes\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 1. What is Logistic Regression?\n",
        "\n",
        "Logistic Regression is a supervised learning algorithm used for **binary classification** tasks, where the goal is to predict one of two possible discrete outcomes (e.g., yes/no, pass/fail).  \n",
        "Instead of predicting continuous values, it estimates the **probability** that a given input belongs to a certain class.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 2. Logistic Regression Equation:\n",
        "\n",
        "The model calculates a weighted sum of the inputs and passes it through a **sigmoid function** to map the result into a probability between 0 and 1.\n",
        "\n",
        "$$\n",
        "z = wX + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **X** is the input feature vector,  \n",
        "* **w** is the weight vector (coefficients),  \n",
        "* **b** is the bias (intercept),  \n",
        "* **z** is the linear combination of inputs and weights,  \n",
        "* **σ (sigmoid)** squashes any real number to a value between 0 and 1, representing probability,  \n",
        "* **$\\hat{y}$** is the predicted probability of the positive class.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 3. How Predictions Work:\n",
        "\n",
        "* If $\\hat{y} \\geq 0.5$, predict class 1 (positive class).  \n",
        "* If $\\hat{y} < 0.5$, predict class 0 (negative class).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 4. Loss Function – Binary Cross-Entropy (Log Loss):\n",
        "\n",
        "The model is trained by minimizing the **binary cross-entropy loss**, which measures how well the predicted probabilities match the true labels:\n",
        "\n",
        "$$\n",
        "L = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$ is the actual label (0 or 1),  \n",
        "* $\\hat{y}_i$ is the predicted probability for sample $i$,  \n",
        "* $n$ is the number of samples.\n",
        "\n",
        "This loss penalizes confident but wrong predictions heavily.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 5. Training via Gradient Descent:\n",
        "\n",
        "* The model updates weights $w$ and bias $b$ by calculating gradients of the loss with respect to these parameters.  \n",
        "* Gradients are averaged over all training samples.  \n",
        "* Parameters are updated in the direction that reduces the loss:\n",
        "\n",
        "$$\n",
        "w := w - \\alpha \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b := b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "Where $\\alpha$ is the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 6. Outcomes of Logistic Regression:\n",
        "\n",
        "* ✅ **Predicted probabilities ($\\hat{y}$)** indicating the likelihood of belonging to class 1.  \n",
        "* ✅ **Class labels (0 or 1)** after applying a threshold (commonly 0.5).  \n",
        "* ✅ **Weights (coefficients)** showing the influence of each feature on the prediction.  \n",
        "* ✅ **Bias (intercept)** shifts the decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 7. Evaluation Metrics for Classification:\n",
        "\n",
        "Common metrics to evaluate logistic regression:\n",
        "\n",
        "* ✅ **Accuracy:** Percentage of correct predictions.  \n",
        "* ✅ **Precision:** Proportion of positive identifications that were actually correct.  \n",
        "* ✅ **Recall (Sensitivity):** Proportion of actual positives identified correctly.  \n",
        "* ✅ **F1 Score:** Harmonic mean of precision and recall.  \n",
        "* ✅ **ROC-AUC:** Area under the Receiver Operating Characteristic curve, showing tradeoff between true positive rate and false positive rate.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 8. Advantages of Logistic Regression:\n",
        "\n",
        "* Simple and interpretable model.  \n",
        "* Outputs probabilities, useful for uncertainty estimation.  \n",
        "* Efficient and fast to train.  \n",
        "* Works well for linearly separable classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 9. Limitations:\n",
        "\n",
        "* Assumes linear decision boundary in feature space.  \n",
        "* Can underperform on complex, non-linear problems.  \n",
        "* Sensitive to irrelevant features and multicollinearity.  \n",
        "* Not suitable for multi-class problems without extension (like one-vs-rest).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 10. When to Use Logistic Regression?\n",
        "\n",
        "* Binary classification problems with interpretable output.  \n",
        "* When probabilities are needed, not just hard labels.  \n",
        "* When dataset is relatively small or features are few.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 11. Summary Table – Linear vs Logistic Regression\n",
        "\n",
        "| Aspect          | Linear Regression               | Logistic Regression               |\n",
        "| --------------- | ------------------------------ | -------------------------------- |\n",
        "| Problem Type    | Regression (continuous output) | Classification (binary output)   |\n",
        "| Output          | Continuous values              | Probability (0 to 1)             |\n",
        "| Model Equation  | $y = a + bX + \\epsilon$        | $\\hat{y} = \\sigma(wX + b)$       |\n",
        "| Loss Function   | Mean Squared Error (MSE)       | Binary Cross-Entropy Loss        |\n",
        "| Prediction Rule | Direct output                  | Threshold at 0.5 for class label |\n",
        "| Use Cases       | Predicting prices, temperature | Spam detection, medical diagnosis|\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PcdixkqamV3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression from Scratch - Code Explanation\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Importing Required Library\n",
        "\n",
        "```python\n",
        "import math\n"
      ],
      "metadata": {
        "id": "WQwMLxaejeVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **math** library for mathematical functions like exponentiation (`exp`) and logarithms (`log`).\n"
      ],
      "metadata": {
        "id": "nBJnrGLljj-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sample Training Data"
      ],
      "metadata": {
        "id": "r-DQJFxDjsyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [0, 0, 0, 1, 1]\n"
      ],
      "metadata": {
        "id": "iBR4_rNbjqib"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- x_data represents the feature values — in this case, hours studied.\n",
        "\n",
        "- y_data is the label or target — 1 means pass, 0 means fail.\n",
        "\n",
        "- This is a binary classification problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "J4MLAZcfjxUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Initializing Parameters"
      ],
      "metadata": {
        "id": "d6wKvzmwj2DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 0.0\n",
        "b = 0.0\n"
      ],
      "metadata": {
        "id": "hUlEB7nCjqfr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **w** is the weight (coefficient) for the feature.  \n",
        "- **b** is the bias (intercept).  \n",
        "- Both start from zero.\n"
      ],
      "metadata": {
        "id": "U0VeM4xdj-3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Sigmoid Function"
      ],
      "metadata": {
        "id": "Rk5VuY2kkA2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + math.exp(-z))\n"
      ],
      "metadata": {
        "id": "I5TF7iFfjqda"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The sigmoid function squashes any real number into a value between 0 and\n",
        "- Used to interpret the output as a probability of belonging to class 1."
      ],
      "metadata": {
        "id": "5ksSPNWAkEb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Parameters"
      ],
      "metadata": {
        "id": "GplpRyIfkJMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "n = len(x_data)\n"
      ],
      "metadata": {
        "id": "iRRiSL8NjqbD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **learning_rate** controls the step size in the gradient descent update.  \n",
        "- **epochs** is how many times the training loop runs.  \n",
        "- **n** is the number of training samples.\n"
      ],
      "metadata": {
        "id": "ISqTCDMTkQru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Loop"
      ],
      "metadata": {
        "id": "3LlgjBpzkSuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        x = x_data[i]\n",
        "        y = y_data[i]\n",
        "\n",
        "        z = w * x + b\n",
        "        y_hat = sigmoid(z)\n",
        "\n",
        "        error = y_hat - y\n",
        "\n",
        "        dw += error * x\n",
        "        db += error\n",
        "\n",
        "        loss += - (y * math.log(y_hat + 1e-8) + (1 - y) * math.log(1 - y_hat + 1e-8))\n",
        "\n",
        "    dw /= n\n",
        "    db /= n\n",
        "    loss /= n\n",
        "\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss:.4f} | w: {w:.4f} | b: {b:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb97ribhjqYY",
        "outputId": "4c1d1788-ad06-49f7-ef1c-4a940ccbaad2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6931 | w: 0.0300 | b: -0.0100\n",
            "Epoch 100 | Loss: 0.4680 | w: 0.4825 | b: -1.3860\n",
            "Epoch 200 | Loss: 0.3649 | w: 0.7567 | b: -2.3566\n",
            "Epoch 300 | Loss: 0.3062 | w: 0.9639 | b: -3.0914\n",
            "Epoch 400 | Loss: 0.2680 | w: 1.1307 | b: -3.6840\n",
            "Epoch 500 | Loss: 0.2410 | w: 1.2711 | b: -4.1833\n",
            "Epoch 600 | Loss: 0.2207 | w: 1.3929 | b: -4.6168\n",
            "Epoch 700 | Loss: 0.2047 | w: 1.5009 | b: -5.0016\n",
            "Epoch 800 | Loss: 0.1916 | w: 1.5984 | b: -5.3489\n",
            "Epoch 900 | Loss: 0.1807 | w: 1.6875 | b: -5.6662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each epoch:\n",
        "\n",
        "1. Initialize gradients `dw` (weight gradient) and `db` (bias gradient), and loss accumulator.\n",
        "\n",
        "2. Loop over each training sample:\n",
        "\n",
        "  - Calculate the linear combination:  \n",
        "  $$\n",
        "  z = w \\times x + b\n",
        "  $$\n",
        "\n",
        "- Pass \\( z \\) through sigmoid to get predicted probability \\( \\hat{y} \\).\n",
        "\n",
        "- Compute error:  \n",
        "  $$\n",
        "  \\text{error} = \\hat{y} - y\n",
        "  $$\n",
        "\n",
        "- Accumulate gradients:  \n",
        "  $$\n",
        "  dw += \\text{error} \\times x \\quad \\text{(gradient of loss w.r.t weight)}\n",
        "  $$  \n",
        "  $$\n",
        "  db += \\text{error} \\quad \\text{(gradient of loss w.r.t bias)}\n",
        "  $$\n",
        "\n",
        "- Calculate binary cross-entropy loss and add to total loss:  \n",
        "  $$\n",
        "  \\text{loss} = - \\left( y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y}) \\right)\n",
        "  $$  \n",
        "  *(Add a small number \\(1e^{-8}\\) to prevent \\(\\log(0)\\)).*\n",
        "\n",
        "3. Average gradients and loss by dividing by \\( n \\).\n",
        "\n",
        "4. Update weights and bias using gradient descent:  \n",
        "  $$\n",
        "  w = w - \\text{learning_rate} \\times dw\n",
        "  $$  \n",
        "  $$\n",
        "  b = b - \\text{learning_rate} \\times db\n",
        "  $$\n",
        "\n",
        "5. Print the loss and parameters every 100 epochs to monitor training.\n"
      ],
      "metadata": {
        "id": "pU6XueDOkdMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Prediction Function"
      ],
      "metadata": {
        "id": "GGsG6Bamk_lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    y_prob = sigmoid(w * x + b)\n",
        "    return 1 if y_prob >= 0.5 else 0\n"
      ],
      "metadata": {
        "id": "SiCVpPULk8tk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an input `x`, calculate the predicted probability using current `w` and `b`.\n",
        "\n",
        "If the probability is greater than or equal to 0.5, classify as **1** (pass), else **0** (fail).\n"
      ],
      "metadata": {
        "id": "JSdKDMr7lHpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Testing Predictions"
      ],
      "metadata": {
        "id": "Wbi5TnnVlIrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting predictions:\")\n",
        "for x in [1, 2, 3, 4, 5, 6]:\n",
        "    print(f\"Input: {x} => Prediction: {predict(x)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlPNwmek8qX",
        "outputId": "55eeff2a-fd59-4c70-8c87-8246a4d6947c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing predictions:\n",
            "Input: 1 => Prediction: 0\n",
            "Input: 2 => Prediction: 0\n",
            "Input: 3 => Prediction: 0\n",
            "Input: 4 => Prediction: 1\n",
            "Input: 5 => Prediction: 1\n",
            "Input: 6 => Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the trained model on inputs 1 through 6.\n",
        "\n",
        "Prints predicted class for each input."
      ],
      "metadata": {
        "id": "CvnhqPJBlORh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- This code implements logistic regression without libraries.\n",
        "- Uses gradient descent to optimize weights and bias.\n",
        "- Uses sigmoid activation to output probabilities.\n",
        "- Uses binary cross-entropy as loss function.\n",
        "- Updates parameters iteratively to minimize loss.\n",
        "- Finally, it predicts class labels based on thresholding predicted probabilities.\n"
      ],
      "metadata": {
        "id": "iUCSsndylXM4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCG_A3X2lQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpce-iWvk8np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9rZtcz0Mqx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}